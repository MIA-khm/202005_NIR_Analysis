{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fundamentals\n",
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(55)\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline \n",
    "\n",
    "#Machine Learning Algorithms\n",
    "\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Multiprocessing\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.7.7\n",
      "numpy 1.18.1\n",
      "pandas 1.0.3\n",
      "sklearn 0.22.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\HyewonKwak\\\\PycharmProjects\\\\Data'"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Versions\n",
    "print('python', sys.version.split(' ')[0])\n",
    "print('numpy', np.__version__)\n",
    "print('pandas', pd.__version__)\n",
    "print('sklearn', sklearn.__version__)\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n"
     ]
    }
   ],
   "source": [
    "#Encoding Check\n",
    "print(sys.getdefaultencoding())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load datasets\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "train = pd.read_csv('train.csv', index_col='id')\n",
    "test = pd.read_csv('test.csv', index_col='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "#직관적인 시각화를 위해서 Indication Dictionary\n",
    "indicator_dict = {\n",
    "    'length' : ['rho'],\n",
    "    'source' : ['650_src','660_src','670_src','680_src','690_src','700_src','710_src','720_src','730_src','740_src','750_src','760_src'\n",
    "               ,'780_src','790_src','800_src','810_src','820_src','830_src','840_src','850_src','860_src','870_src','880_src','890_src',\n",
    "               '900_src','910_src','920_src','930_src','940_src','950_src','960_src','970_src','980_src','990_src'],\n",
    "    'detect' : ['650_dst',\n",
    "               '660_dst', '670_dst', '680_dst', '690_dst', '700_dst', '710_dst',\n",
    "               '720_dst', '730_dst', '740_dst', '750_dst', '760_dst', '770_dst',\n",
    "               '780_dst', '790_dst', '800_dst', '810_dst', '820_dst', '830_dst',\n",
    "               '840_dst', '850_dst', '860_dst', '870_dst', '880_dst', '890_dst',\n",
    "               '900_dst', '910_dst', '920_dst', '930_dst', '940_dst', '950_dst',\n",
    "               '960_dst', '970_dst', '980_dst', '990_dst']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train 데이터에서 예측해야할 hbo2 hbb ca na\n",
    "\n",
    "train_X=train[train.columns[:71]].copy()\n",
    "train_Y=train[train.columns[71:]].copy()\n",
    "\n",
    "\n",
    "test_X = test.copy()\n",
    "\n",
    "combined_X = pd.concat([train_X,test_X],axis=0)\n",
    "X_list = [train_X,test_X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140168\n",
      "70168\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리하기\n",
    "print(combined_X.isnull().sum().sum())\n",
    "print(train_X.isnull().sum().sum())\n",
    "print(train_Y.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "#아웃라이어 제거\n",
    "#def del_outlier(x):\n",
    "#    from_out = x.mean() - 3 * x.std()\n",
    "#    to_out = x.mean() + 3 * x.std()\n",
    "    \n",
    "#    x = x[(x>from_out) & (x<to_out)]\n",
    "    \n",
    "#    return x#\n",
    "#train_X=del_outlier(train_X)\n",
    "\n",
    "#test_X=del_outlier(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_columns = [k for k in train_X.columns if 'src' in k]\n",
    "dst_columns = [k for k in train_X.columns if 'dst' in k]\n",
    "od_columns = [k for k in train_X.columns if 'od' in k]\n",
    "\n",
    "for X in X_list:\n",
    "    X_dst=X[dst_columns]\n",
    "    X[dst_columns]=X_dst.interpolate(axis=1)\n",
    "    X.fillna(method='bfill',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X in X_list:\n",
    "    X['i0']=np.sum(X[X.columns[X.columns.str.contains('src')]],axis=1)\n",
    "    X['i1']=np.sum(X[X.columns[X.columns.str.contains('dst')]],axis=1)\n",
    "    X[subtract_col]=np.subtract(X[src_columns],X[dst_columns])\n",
    "    X['re1']=-np.log((X['i0']-X['i1'])/X['i0'])#Od\n",
    "    X['re2']=X['i0']-X['i1']\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for X in X_list:\n",
    "    #흡수계수\n",
    "    X['S_ac'] = np.log(X['rho']*X['rho']*X['re2'])/X['rho']\n",
    "    #위상천이변화율\n",
    "    X['S_pi'] =X['re1']/X['rho']\n",
    "    \n",
    "    X['ab']=X['S_ac']/X['S_pi']-X['S_pi']/X['S_ac'] \n",
    "    X['di']=((X['S_ac']*X['S_ac'])-(X['S_pi']*X['S_pi']))/(3*X['ab'])-X['ab']\n",
    "\n",
    "    absorb_coef=(990/650)*(990/650)*(990/650)*(990/650)\n",
    "    r=650\n",
    "    X['absorb']=absorb_coef*X['rho']\n",
    "    for k in range(0,35):\n",
    "        name= str(r)+'_od'\n",
    "        name2=str(r)+'_od2'\n",
    "        name1=str(r)+'_od1'\n",
    "        X[name]=-np.log(X[str(r)+'_dst']/X[str(r)+'_src'])\n",
    "        X[name2]= 223.3+0.05624*np.power(X[name],0.8)+ 0.001245*r*r - 0.9025*r\n",
    "        X[name1]=X[name]/(X[name2]*X['rho'])\n",
    "        r+=10  \n",
    "    X.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "    X.interpolate(axis=1, inplace=True)\n",
    "    X.fillna(method='bfill',inplace=True)\n",
    "    #X.drop(src_columns,axis=1,inplace=True)\n",
    "   # X.drop(dst_columns,axis=1,inplace=True)\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rho</th>\n",
       "      <th>650_src</th>\n",
       "      <th>660_src</th>\n",
       "      <th>670_src</th>\n",
       "      <th>680_src</th>\n",
       "      <th>690_src</th>\n",
       "      <th>700_src</th>\n",
       "      <th>710_src</th>\n",
       "      <th>720_src</th>\n",
       "      <th>730_src</th>\n",
       "      <th>...</th>\n",
       "      <th>960_od1</th>\n",
       "      <th>970_od</th>\n",
       "      <th>970_od2</th>\n",
       "      <th>970_od1</th>\n",
       "      <th>980_od</th>\n",
       "      <th>980_od2</th>\n",
       "      <th>980_od1</th>\n",
       "      <th>990_od</th>\n",
       "      <th>990_od2</th>\n",
       "      <th>990_od1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.37950</td>\n",
       "      <td>0.42993</td>\n",
       "      <td>0.52076</td>\n",
       "      <td>0.57166</td>\n",
       "      <td>0.67818</td>\n",
       "      <td>0.75476</td>\n",
       "      <td>0.83580</td>\n",
       "      <td>0.93623</td>\n",
       "      <td>0.96333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>38.780756</td>\n",
       "      <td>520.344893</td>\n",
       "      <td>0.002981</td>\n",
       "      <td>37.100948</td>\n",
       "      <td>535.560869</td>\n",
       "      <td>0.002771</td>\n",
       "      <td>34.773339</td>\n",
       "      <td>551.011206</td>\n",
       "      <td>0.002524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01813</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01974</td>\n",
       "      <td>0.00321</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003927</td>\n",
       "      <td>18.661584</td>\n",
       "      <td>519.880026</td>\n",
       "      <td>0.003590</td>\n",
       "      <td>17.994615</td>\n",
       "      <td>535.115753</td>\n",
       "      <td>0.003363</td>\n",
       "      <td>17.828324</td>\n",
       "      <td>550.613052</td>\n",
       "      <td>0.003238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03289</td>\n",
       "      <td>0.02416</td>\n",
       "      <td>0.03610</td>\n",
       "      <td>0.05843</td>\n",
       "      <td>0.09015</td>\n",
       "      <td>0.14944</td>\n",
       "      <td>0.18578</td>\n",
       "      <td>0.25584</td>\n",
       "      <td>...</td>\n",
       "      <td>28.443879</td>\n",
       "      <td>30.813912</td>\n",
       "      <td>33.183945</td>\n",
       "      <td>35.553978</td>\n",
       "      <td>37.924011</td>\n",
       "      <td>535.578805</td>\n",
       "      <td>0.002832</td>\n",
       "      <td>37.699338</td>\n",
       "      <td>551.075417</td>\n",
       "      <td>0.002736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.27503</td>\n",
       "      <td>0.31281</td>\n",
       "      <td>0.32898</td>\n",
       "      <td>0.41041</td>\n",
       "      <td>0.46587</td>\n",
       "      <td>0.52769</td>\n",
       "      <td>0.64369</td>\n",
       "      <td>0.73562</td>\n",
       "      <td>0.79865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004152</td>\n",
       "      <td>19.754746</td>\n",
       "      <td>519.907262</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>19.365963</td>\n",
       "      <td>535.150111</td>\n",
       "      <td>0.003619</td>\n",
       "      <td>18.860697</td>\n",
       "      <td>550.639010</td>\n",
       "      <td>0.003425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.01521</td>\n",
       "      <td>1.00872</td>\n",
       "      <td>0.98930</td>\n",
       "      <td>0.98874</td>\n",
       "      <td>1.01773</td>\n",
       "      <td>1.01632</td>\n",
       "      <td>1.00009</td>\n",
       "      <td>0.98217</td>\n",
       "      <td>1.01564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>29.634120</td>\n",
       "      <td>520.141712</td>\n",
       "      <td>0.003798</td>\n",
       "      <td>29.647138</td>\n",
       "      <td>535.394509</td>\n",
       "      <td>0.003692</td>\n",
       "      <td>29.662270</td>\n",
       "      <td>550.896355</td>\n",
       "      <td>0.003590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 220 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rho  650_src  660_src  670_src  680_src  690_src  700_src  710_src  \\\n",
       "id                                                                        \n",
       "0   25.0  0.37950  0.42993  0.52076  0.57166  0.67818  0.75476  0.83580   \n",
       "1   10.0  0.00000  0.00000  0.01813  0.00000  0.00000  0.01974  0.00321   \n",
       "2   25.0  0.00000  0.03289  0.02416  0.03610  0.05843  0.09015  0.14944   \n",
       "3   10.0  0.27503  0.31281  0.32898  0.41041  0.46587  0.52769  0.64369   \n",
       "4   15.0  1.01521  1.00872  0.98930  0.98874  1.01773  1.01632  1.00009   \n",
       "\n",
       "    720_src  730_src  ...    960_od1     970_od     970_od2    970_od1  \\\n",
       "id                    ...                                                \n",
       "0   0.93623  0.96333  ...   0.003155  38.780756  520.344893   0.002981   \n",
       "1   0.00000  0.00000  ...   0.003927  18.661584  519.880026   0.003590   \n",
       "2   0.18578  0.25584  ...  28.443879  30.813912   33.183945  35.553978   \n",
       "3   0.73562  0.79865  ...   0.004152  19.754746  519.907262   0.003800   \n",
       "4   0.98217  1.01564  ...   0.003937  29.634120  520.141712   0.003798   \n",
       "\n",
       "       980_od     980_od2   980_od1     990_od     990_od2   990_od1  \n",
       "id                                                                    \n",
       "0   37.100948  535.560869  0.002771  34.773339  551.011206  0.002524  \n",
       "1   17.994615  535.115753  0.003363  17.828324  550.613052  0.003238  \n",
       "2   37.924011  535.578805  0.002832  37.699338  551.075417  0.002736  \n",
       "3   19.365963  535.150111  0.003619  18.860697  550.639010  0.003425  \n",
       "4   29.647138  535.394509  0.003692  29.662270  550.896355  0.003590  \n",
       "\n",
       "[5 rows x 220 columns]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_data, y_data, r, k=5):\n",
    "    models=[]\n",
    "    \n",
    "    k_fold = KFold(n_splits=k, shuffle=True, random_state=123)\n",
    "    \n",
    "    for train_idx, val_idx in k_fold.split(x_data):\n",
    "        x_train, y_train = x_data.iloc[train_idx], y_data[train_idx]\n",
    "        x_val, y_val = x_data.iloc[val_idx], y_data[val_idx]\n",
    "    \n",
    "        d_train = xgb.DMatrix(data = x_train, label = y_train)\n",
    "        d_val = xgb.DMatrix(data = x_val, label = y_val)\n",
    "        \n",
    "        wlist = [(d_train, 'train'), (d_val, 'eval')]\n",
    "        \n",
    "        params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'eval_metric': 'mae',\n",
    "            'seed':777\n",
    "            }\n",
    "        \n",
    "        model = xgb.train(params=params, dtrain=d_train, num_boost_round=r, verbose_eval=r, evals=wlist)\n",
    "        models.append(model)\n",
    "    \n",
    "    return models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train column :  hhb\n",
      "[0]\ttrain-mae:5.26494\teval-mae:5.24360\n",
      "[99]\ttrain-mae:0.25029\teval-mae:0.81225\n",
      "[0]\ttrain-mae:5.27133\teval-mae:5.22464\n",
      "[99]\ttrain-mae:0.23879\teval-mae:0.77604\n",
      "[0]\ttrain-mae:5.26438\teval-mae:5.26769\n",
      "[99]\ttrain-mae:0.24071\teval-mae:0.78793\n",
      "[0]\ttrain-mae:5.25909\teval-mae:5.30563\n",
      "[99]\ttrain-mae:0.24952\teval-mae:0.81065\n",
      "[0]\ttrain-mae:5.25668\teval-mae:5.30683\n",
      "[99]\ttrain-mae:0.23375\teval-mae:0.80075\n",
      "train column :  hbo2\n",
      "[0]\ttrain-mae:2.46038\teval-mae:2.47480\n",
      "[99]\ttrain-mae:0.20655\teval-mae:0.60325\n",
      "[0]\ttrain-mae:2.46009\teval-mae:2.47434\n",
      "[99]\ttrain-mae:0.20677\teval-mae:0.60090\n",
      "[0]\ttrain-mae:2.46920\teval-mae:2.42796\n",
      "[99]\ttrain-mae:0.20395\teval-mae:0.60949\n",
      "[0]\ttrain-mae:2.45957\teval-mae:2.47788\n",
      "[99]\ttrain-mae:0.19706\teval-mae:0.60662\n",
      "[0]\ttrain-mae:2.46431\teval-mae:2.45680\n",
      "[99]\ttrain-mae:0.19492\teval-mae:0.62432\n",
      "train column :  ca\n",
      "[0]\ttrain-mae:5.99003\teval-mae:6.08417\n",
      "[99]\ttrain-mae:0.69466\teval-mae:1.96914\n",
      "[0]\ttrain-mae:5.99177\teval-mae:6.00726\n",
      "[99]\ttrain-mae:0.64665\teval-mae:1.95014\n",
      "[0]\ttrain-mae:5.99696\teval-mae:6.06855\n",
      "[99]\ttrain-mae:0.65797\teval-mae:1.93021\n",
      "[0]\ttrain-mae:6.01717\teval-mae:5.86548\n",
      "[99]\ttrain-mae:0.67484\teval-mae:1.88501\n",
      "[0]\ttrain-mae:6.00138\teval-mae:5.97678\n",
      "[99]\ttrain-mae:0.71599\teval-mae:1.95549\n",
      "train column :  na\n",
      "[0]\ttrain-mae:2.05733\teval-mae:1.95515\n",
      "[99]\ttrain-mae:0.42257\teval-mae:1.24943\n",
      "[0]\ttrain-mae:2.03099\teval-mae:2.03933\n",
      "[99]\ttrain-mae:0.41436\teval-mae:1.25750\n",
      "[0]\ttrain-mae:2.01519\teval-mae:2.12041\n",
      "[99]\ttrain-mae:0.40560\teval-mae:1.22090\n",
      "[0]\ttrain-mae:2.03850\teval-mae:2.04060\n",
      "[99]\ttrain-mae:0.43521\teval-mae:1.25933\n",
      "[0]\ttrain-mae:2.02578\teval-mae:2.07862\n",
      "[99]\ttrain-mae:0.41727\teval-mae:1.27700\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "#for label in train_Y.columns[range(2)]:\n",
    "for label in train_Y.columns:\n",
    "    print('train column : ', label)\n",
    "    models[label] = train_model(train_X, train_Y[label],r=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame()\n",
    "df_test = pd.DataFrame()\n",
    "\n",
    "for col in models:\n",
    "    preds_train = []\n",
    "    preds_test = []\n",
    "    for model in models[col]:\n",
    "        preds_train.append(model.predict(xgb.DMatrix(train_X)))\n",
    "        preds_test.append(model.predict(xgb.DMatrix(test_X)))\n",
    "\n",
    "    pred_train = np.mean(preds_train, axis=0)\n",
    "    pred_test = np.mean(preds_test, axis=0)\n",
    "    \n",
    "    df_train[col] = pred_train\n",
    "    df_test[col] = pred_test\n",
    "\n",
    "    sample_submission[col] = pred_test\n",
    "    sample_submission.to_csv('data_0620.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting train2_Ys, test_Ys, and coefficients of mapping X -> Ys\n",
    "def X_Ys_training(train1_X, train1_Ys, train2_X, test_X, day_index, night_index, multi=False):\n",
    "    # multi_model에서 day, night index를 담는 grid. single model에서는 활용하지 않음.\n",
    "    if multi:\n",
    "        # day-night 경계에서 보다 seamless한 추정을 하도록 multiple index에 대해 training한 후 평균을 냄. \n",
    "        # multiple index는 day_index, night_index 기준으로 앞뒤로 30분, 60분 지점들의 combination을 meshgrid 형태로 저장함. (5*5=25 models)\n",
    "        day_grid = np.meshgrid(np.arange(day_index-6, day_index+7, 3), np.arange(night_index-6, night_index+7, 3))[0].reshape(-1)\n",
    "        night_grid = np.meshgrid(np.arange(day_index-6, day_index+7, 3), np.arange(night_index-6, night_index+7, 3))[1].reshape(-1)\n",
    "        n_models = day_grid.shape[0]  # n_models=25\n",
    "    else:\n",
    "        day_grid, night_grid = [day_index], [night_index]\n",
    "        n_models = 1\n",
    "    \n",
    "    # Ys->Y18 학습에 필요한 train2_Ys, test_Ys 결과 값을 저장할 DataFrame\n",
    "    train2_Ys = pd.DataFrame(np.zeros((train2_X.shape[0], train1_Ys.shape[1])), index=train2_X.index, columns=train1_Ys.columns)\n",
    "    test_Ys = pd.DataFrame(np.zeros((test_X.shape[0], train1_Ys.shape[1])), index=test_X.index, columns=train1_Ys.columns)\n",
    "    \n",
    "    # Y00부터 Y17까지의 X->Y mapping의 coefficients를 저장하는 리스트. 각 coefficients는 index가 time(0~144)이고 columns가 X(X00, X02, ... , neg_insol)인 DataFrame.\n",
    "    X_Ys_coefs_list = []\n",
    "    for i in range(len(train1_Ys.columns)):\n",
    "        coefs_df = pd.DataFrame(np.zeros((144, train1_X.shape[1])), index=np.arange(144), columns=train1_X.columns)\n",
    "        coefs_df['Ys_intercept'] = 0  # X->Ys에 대한 intercept column\n",
    "        X_Ys_coefs_list.append(coefs_df)\n",
    "    \n",
    "    # Ys를 구하는 model은 효율을 위해 LassoCV가 아닌 Lasso를 적용.\n",
    "    lasso = Lasso(alpha=0.05)\n",
    "    \n",
    "    # n_models개의 Day, night index의 조합에 대해 각각 model을 학습해 coefficients를 구함. (5*5=25 models)\n",
    "    for day, night in zip(day_grid, night_grid):\n",
    "        tr1_day_ids, tr1_night_ids = day_night_split(train1_X, day, night)\n",
    "        day_train1_X, night_train1_X = train1_X.loc[tr1_day_ids], train1_X.loc[tr1_night_ids]\n",
    "        \n",
    "        tr2_day_ids, tr2_night_ids = day_night_split(train2_X, day, night)\n",
    "        day_train2_X, night_train2_X = train2_X.loc[tr2_day_ids], train2_X.loc[tr2_night_ids]\n",
    "                \n",
    "        ts_day_ids, ts_night_ids = day_night_split(test_X, day, night)\n",
    "        day_test_X, night_test_X = test_X.loc[ts_day_ids], test_X.loc[ts_night_ids]\n",
    "        \n",
    "        # 낮 시간대에서 X12(humidity), X20(humidity)가 noisy 하기 때문에 drop.\n",
    "        for day_X in [day_train1_X, day_train2_X, day_test_X]:\n",
    "            day_X.drop(['X12', 'X20'], axis=1, inplace=True)\n",
    "            \n",
    "        # 밤 시간대에서 X31(temperature), X38(humidity)가 noisy 하기 때문에 drop\n",
    "        for night_X in [night_train1_X, night_train2_X, night_test_X]:\n",
    "            night_X.drop(['X31', 'X38'], axis=1, inplace=True)\n",
    "        \n",
    "        # Y00부터 Y17까지 Loop\n",
    "        for i, c_name in enumerate(train1_Ys.columns):\n",
    "            # ----------- Day training -----------\n",
    "            day_train1_Y = train1_Ys.loc[tr1_day_ids, c_name]\n",
    "            lasso.fit(day_train1_X, day_train1_Y)\n",
    "            \n",
    "            # day_time에 대한 coefficients를 X_Ys_coefs_list에 저장. n개의 model에서의 coefficients들의 평균을 얻기 위해 n_models로 나누어줌.\n",
    "            day_time = range(day+1, night+1)\n",
    "            coefs = np.tile(lasso.coef_, (len(day_time), 1)) / n_models\n",
    "            X_Ys_coefs_list[i].loc[day_time, day_train1_X.columns] += coefs\n",
    "            X_Ys_coefs_list[i].loc[day_time, 'Ys_intercept'] += lasso.intercept_ / n_models\n",
    "            \n",
    "            # train2_Ys, test_Ys Predict. n개의 model에서의 prediction 값의 평균을 얻기 위해 n_models로 나누어줌.\n",
    "            train2_Ys.loc[tr2_day_ids, c_name] += lasso.predict(day_train2_X) / n_models\n",
    "            test_Ys.loc[ts_day_ids, c_name] += lasso.predict(day_test_X) / n_models\n",
    "            \n",
    "            # ----------- Night training -----------\n",
    "            # Same as Day training\n",
    "            night_train1_Y = train1_Ys.loc[tr1_night_ids, c_name]\n",
    "            lasso.fit(night_train1_X, night_train1_Y)\n",
    "            \n",
    "            night_time = list(range(0, day+1)) + list(range(night+1, 144))\n",
    "            coefs = np.tile(lasso.coef_, (len(night_time), 1)) / n_models\n",
    "            X_Ys_coefs_list[i].loc[night_time, night_train1_X.columns] += coefs\n",
    "            X_Ys_coefs_list[i].loc[night_time, 'Ys_intercept'] += lasso.intercept_ / n_models\n",
    "            \n",
    "            train2_Ys.loc[tr2_night_ids, c_name] += lasso.predict(night_train2_X) / n_models\n",
    "            test_Ys.loc[ts_night_ids, c_name] += lasso.predict(night_test_X) / n_models\n",
    "    \n",
    "    return train2_Ys, test_Ys, X_Ys_coefs_list\n",
    "\n",
    "\n",
    "# train2_Y18에 대해 correlation 값이 상위 n개인 Y들을 구함 -> Ys_high_correlated\n",
    "def high_corr_Ys(train_Y, n=None):\n",
    "    if n==None:\n",
    "        n = train_Y.shape[1]\n",
    "        \n",
    "    corrs = pd.DataFrame(index=train_Y.columns, columns=['corr'])\n",
    "    for c in train_Y:\n",
    "        corrs.loc[c, 'corr'] = np.corrcoef(train_Y[c], train2_Y18)[0,1]\n",
    "    corrs = corrs.sort_values(by='corr', ascending=False)\n",
    "    \n",
    "    Ys_high_correlated = corrs[corrs.columns[:n]].index\n",
    " \n",
    "    return Ys_high_correlated\n",
    "\n",
    "\n",
    "# Function for getting coefficients of mapping Ys -> Y18\n",
    "def Ys_Y18_training(train1_X, train2_Ys, train2_Y18, test_Ys, X_Ys_coefs_list, day_index, night_index):\n",
    "    tr2_day_ids, tr2_night_ids = day_night_split(train2_Ys, day_index, night_index)\n",
    "    day_train2_Ys, night_train2_Ys = train2_Ys.loc[tr2_day_ids], train2_Ys.loc[tr2_night_ids]\n",
    "    day_train2_Y18, night_train2_Y18 = train2_Y18.loc[tr2_day_ids], train2_Y18.loc[tr2_night_ids]\n",
    "    \n",
    "    # X->Y18 mapping의 coefficients를 저장하는 DataFrame. index는 time(0~144)이고 columns는 X(X00, X02, ... , neg_insol)임.\n",
    "    X_Y18_coefs = pd.DataFrame(np.zeros((144, train1_X.shape[1])), index=np.arange(144), columns=train1_X.columns)\n",
    "    X_Y18_coefs['Ys_intercept'] = 0  # X->Ys 에서의 intercept에 대한 coefficient\n",
    "    X_Y18_coefs['Y18_intercept'] = 0  # Ys->Y18 에서의 intercept에 대한 coefficient \n",
    "    \n",
    "    # Day, Night index에 따라 optimal alpha 값이 달라지므로, LassoCV를 이용함.\n",
    "    model = LassoCV(eps=0.01, n_alphas=40, cv=5, n_jobs=-1)\n",
    "    \n",
    "    # Day training\n",
    "    day_Ys_high_correlated = high_corr_Ys(day_train2_Ys, day_train2_Y18, n=10)  # correlation이 높음 10개의 Y로 training을 진행.\n",
    "    day_train2_Ys = day_train2_Ys[day_Ys_high_correlated]\n",
    "    model.fit(day_train2_Ys, day_train2_Y18)\n",
    "    \n",
    "    # Ys->Y18에 더함.\n",
    "    # 예를 들어, Ys->Y18의 coefficients가 {'Y00': 0, Y01': 1.2, 'Y02': 0.1} 이라고 하면 X_Y18_coefs는 아래와 같이 구할 수 있음.\n",
    "    # X_Y18_coefs = (X_Ys_coefs_list[0] * 0) + (X_Ys_coefs_list[1] * 1.2) + (X_Ys_coefs_list[2] * 0.1) \n",
    "    # day와 night에 각각에 대해 X_Y18_coefs를 계산한다.\n",
    "    Ys_Y18_coefs = pd.Series(model.coef_, index=day_train2_Ys.columns)\n",
    "    Ys_Y18_coefs = Ys_Y18_coefs.loc[Ys_Y18_coefs!=0]\n",
    "    day_time = np.arange(day_index+1, night_index+1)\n",
    "    for Y in Ys_Y18_coefs.index:\n",
    "        idx = np.where(train2_Ys.columns==Y)[0][0]\n",
    "        X_Y18_coefs.loc[day_time, X_Y18_coefs.columns[:-1]] += X_Ys_coefs_list[idx].loc[day_time] * Ys_Y18_coefs.loc[Y]\n",
    "        X_Y18_coefs.loc[day_time, 'Y18_intercept'] = model.intercept_\n",
    "    \n",
    "    # Night training (Same as Day training)\n",
    "    night_Ys_high_correlated = high_corr_Ys(night_train2_Ys, night_train2_Y18, n=10)\n",
    "    night_train2_Ys = night_train2_Ys[night_Ys_high_correlated]\n",
    "    model.fit(night_train2_Ys, night_train2_Y18)\n",
    "    \n",
    "    Ys_Y18_coefs = pd.Series(model.coef_, index=night_train2_Ys.columns)\n",
    "    Ys_Y18_coefs = Ys_Y18_coefs.loc[Ys_Y18_coefs!=0]\n",
    "    night_time = list(range(0, day_index+1)) + list(range(night_index+1, 144))\n",
    "    for Y in Ys_Y18_coefs.index:\n",
    "        idx = np.where(train2_Ys.columns==Y)[0][0]\n",
    "        X_Y18_coefs.loc[night_time, X_Y18_coefs.columns[:-1]] += X_Ys_coefs_list[idx].loc[night_time] * Ys_Y18_coefs.loc[Y]\n",
    "        X_Y18_coefs.loc[night_time, 'Y18_intercept'] = model.intercept_\n",
    "    \n",
    "    return X_Y18_coefs\n",
    "\n",
    "\n",
    "# Function for predicting Y18 with X_Y18_coefs. \n",
    "def predict_Y18(train1_X, target_X, X_Y18_coefs):\n",
    "    scaler = StandardScaler().fit(train1_X)\n",
    "    target_X = pd.DataFrame(scaler.transform(target_X), index=target_X.index, columns=target_X.columns)\n",
    "  \n",
    "    # intercept는 변수가 아니므로 1.0으로 고정함. (X_Y18_coefs에 저장된 값을 그대로 사용하기 위해) \n",
    "    target_X['Ys_intercept'] = 1.0\n",
    "    target_X['Y18_intercept'] = 1.0\n",
    "    \n",
    "    target_Y18 = pd.Series(np.zeros((target_X.shape[0])), index=target_X.index)\n",
    "\n",
    "    # time 0부터 143까지, target_X의 시간 t의 데이터와 X_Y18_coefs의 시간 t에 대한 coefficients를 inner product.\n",
    "    for t in range(144):\n",
    "        Y_pred = target_X.loc[(target_X.index%144)==t].dot(X_Y18_coefs.loc[t].values)\n",
    "        target_Y18.loc[Y_pred.index] = Y_pred\n",
    "    \n",
    "    target_X.drop(['Ys_intercept', 'Y18_intercept'], axis=1, inplace=True)\n",
    "    \n",
    "    return target_Y18\n",
    "\n",
    "\n",
    "# model for single day, night index \n",
    "def single_model(train1_X, train1_Ys, train2_X, train2_Y18, test_X, day_night_index, multi=False):\n",
    "    day_index, night_index = day_night_index\n",
    "    print('day_index : %s, night_index : %s' % (day_index, night_index))\n",
    "    # ----------------------- Standard Scaler -----------------------\n",
    "    scaler = StandardScaler().fit(train1_X)\n",
    "    train1_X = pd.DataFrame(scaler.transform(train1_X), index=train1_X.index, columns=train1_X.columns)\n",
    "    train2_X = pd.DataFrame(scaler.transform(train2_X), index=train2_X.index, columns=train2_X.columns)\n",
    "    test_X = pd.DataFrame(scaler.transform(test_X), index=test_X.index, columns=test_X.columns)\n",
    "\n",
    "    # ----------------------- Predict Y00-Y17 -----------------------\n",
    "    train2_Ys, test_Ys, X_Ys_coefs_list = X_Ys_training(train1_X, train1_Ys, train2_X, test_X, day_index, night_index, multi)\n",
    "    \n",
    "    # ----------------------- Predict Y18 -----------------------\n",
    "    X_Y18_coefs = Ys_Y18_training(train1_X, train2_Ys, train2_Y18, test_Ys, X_Ys_coefs_list, day_index, night_index)\n",
    "    \n",
    "    # multi인 경우엔 predict를 하지 않고 coefs만 return함.\n",
    "    if multi:\n",
    "        return X_Y18_coefs\n",
    "    else:\n",
    "        train2_Y18_pred = predict_Y18(train1_X, train2_X, X_Y18_coefs)\n",
    "        test_Y18 = predict_Y18(train1_X, test_X, X_Y18_coefs)\n",
    "        return train2_Y18_pred, test_Y18,  X_Y18_coefs\n",
    "    \n",
    "    \n",
    "single_train2_Y18_pred, single_test_Y18, single_X_Y18_coefs =\\\n",
    "single_model(train1_X, train1_Ys, train2_X, train2_Y18, test_X, day_night_index=(33, 100), multi=False)\n",
    "\n",
    "# single model training error\n",
    "mean_squared_error(single_train2_Y18_pred, train2_Y18)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.lineplot(x=train2_Y18.index, y=train2_Y18, ax=ax, c='k')\n",
    "sns.lineplot(x=train2_Y18.index, y=single_train2_Y18_pred, ax=ax, c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multi_model(day_start, day_end, night_start, night_end, predict=True, **single_model_params):\n",
    "    # day, night index의 grid. 총 ((day_end - day_start) / 2) * ((night_end - night_start) / 2) 개의 models.\n",
    "    day_grid = np.meshgrid(np.arange(day_start, day_end, 2), np.arange(night_start, night_end, 2))[0].reshape(-1)\n",
    "    night_grid = np.meshgrid(np.arange(day_start, day_end, 2), np.arange(night_start, night_end, 2))[1].reshape(-1)\n",
    "    n_models = day_grid.shape[0]\n",
    "    \n",
    "    # Multiprocessing\n",
    "    pool = ThreadPool(8)\n",
    "    X_Y18_coefs_list = pool.map(lambda grid: single_model(day_night_index=grid, multi=True, **single_model_params), zip(day_grid, night_grid))\n",
    "    \n",
    "    # list에 저장된 multiprocessing의 결과(coefficients for each day, night combination)의 평균값을 구함.\n",
    "    multi_X_Y18_coefs = pd.DataFrame(np.zeros((144, train1_X.shape[1])), index=np.arange(144), columns=train1_X.columns)\n",
    "    multi_X_Y18_coefs['Ys_intercept'] = 0\n",
    "    multi_X_Y18_coefs['Y18_intercept'] = 0\n",
    "    for coefs in X_Y18_coefs_list:\n",
    "        multi_X_Y18_coefs += coefs\n",
    "    multi_X_Y18_coefs /= n_models\n",
    "    \n",
    "    # Predict Y18\n",
    "    if predict:\n",
    "        train2_Y18 = predict_Y18(train1_X, train2_X, multi_X_Y18_coefs)\n",
    "        test_Y18 = predict_Y18(train1_X, test_X, multi_X_Y18_coefs)\n",
    "        return train2_Y18, test_Y18, multi_X_Y18_coefs\n",
    "    else:\n",
    "        return multi_X_Y18_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# day_start, day_ends, night_start, night_end 값은 리더보드 점수가 가장 높은 것으로 선택함.\n",
    "train2_Y18_pred, test_Y18, multi_X_Y18_coefs = multi_model(day_start=21, day_end=45, night_start=91, night_end=109, predict=True,\n",
    "                                                           train1_X=train1_X, train1_Ys=train1_Ys, train2_X=train2_X, train2_Y18=train2_Y18, test_X=test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(train2_Y18_pred, train2_Y18)\n",
    "\n",
    "# Visualization : train2_Y18 prediction \n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.lineplot(x=train2_Y18.index, y=train2_Y18, ax=ax, c='k')\n",
    "sns.lineplot(x=train2_Y18.index, y=train2_Y18_pred, ax=ax, c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot describe a DataFrame without columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-1cd412951a00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\Data\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdescribe\u001b[1;34m(self, percentiles, include, exclude)\u001b[0m\n\u001b[0;32m   9838\u001b[0m         \"\"\"\n\u001b[0;32m   9839\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9840\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot describe a DataFrame without columns\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   9841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9842\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpercentiles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot describe a DataFrame without columns"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.concat([train_X,df_train],axis=1)\n",
    "df_test=pd.concat([test_X,df_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train column :  ca\n",
      "[0]\ttrain-mae:5.99076\teval-mae:6.08719\n",
      "[999]\ttrain-mae:0.00169\teval-mae:2.02914\n",
      "[0]\ttrain-mae:5.99972\teval-mae:6.02855\n",
      "[999]\ttrain-mae:0.00143\teval-mae:2.06457\n",
      "[0]\ttrain-mae:5.99876\teval-mae:6.04462\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-8a90a962c730>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mr_sq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mmodels_2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-7fd4d6966120>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(x_data, y_data, r, k)\u001b[0m\n\u001b[0;32m     19\u001b[0m             }\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Data\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    210\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Data\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Data\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1367\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0;32m   1368\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1369\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1370\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models_2 = {}\n",
    "for label in train_Y.columns[range(2,4)]:\n",
    "    print('train column : ', label)\n",
    "    model = LinearRegression()\n",
    "    model.fit(df_train, train_Y)\n",
    "    r_sq = model.score(df_train,train_Y)\n",
    "    models_2[label] = train_model(df_train, train_Y[label],r=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data",
   "language": "python",
   "name": "data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
